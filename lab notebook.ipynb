{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import warnings\n",
    "import sklearn\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "#to print all output from the cell, not just the last line\n",
    "#from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "#InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "#toggle visualizations because they slow things down\n",
    "visualizations = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/online_shoppers_intention.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This classification project examines a dataset of online shopping sessions to predict whether or not the session ended in a purchase. The imagined audience is a web development firm looking to improve the ecommerce functionalities they offer their clients. They are hoping to develop new ecommerce tools and functionalities for their platform. Development of this classification model is the first exploratory step to understand the needs of their clients. This model might pave the way for development and A/B testing of new ecommerce features. \n",
    "\n",
    "This firm would like to demonstrate their return on their clients' investment by demonstrating how they might help their clients convert traffic into sales. Those clients might be small(ish) business owners, who are hoping to optimize their advertising strategies, learn what types of customers to target, and drive overall sales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project, I used a dataset titled \"Online Shoppers Purchasing Intention\" from the UCI Machine Learning Repository. The data was used in a 2018 article on machine learning models' potential for predicting user behavior in ecommerce sites, and was donated to the repository in 2018 \n",
    "\n",
    "After spending some time with the data, I informally organized the 18 features into several categories:\n",
    "\n",
    "<b> Type of pages visited and session duration </b> <br>\n",
    "These features include the number of Administrative, Informational, and Product-Related pages visited in the session, and the total duration spent in each of those categories in seconds.\n",
    "\n",
    "<b> Qualities of pages visited </b> <br>\n",
    "This section includes Google analytics data for the pages visited in this session, aggregated by mean. Includes Bounce Rates (frequency of sessions that enter and exit on a given page), Exit Rates (frequency of sessions that end on this page), and Page Values (rate at which the given page leads to a purchase) for each page.\n",
    "\n",
    "<b> Season session occurred </b> <br>\n",
    "These two features indicate which month the session occurred, and the session's proximity to one of two designated 'Special Day's, or days expected to yield a high number of gift purchases (Valentine's Day and Mother's Day). \n",
    "\n",
    "<b> User information </b> <br>\n",
    "Finally, there was a handful of data about the user collected for each session: the operating system and browser type used to access the site, and the region where the user is located. Additionally, we know how the user came to the site (Traffic Type) and if they had previously visited the site (Visitor Type). Whether the session occurred on a weekend is captured in Weekend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Administrative</th>\n",
       "      <th>Administrative_Duration</th>\n",
       "      <th>Informational</th>\n",
       "      <th>Informational_Duration</th>\n",
       "      <th>ProductRelated</th>\n",
       "      <th>ProductRelated_Duration</th>\n",
       "      <th>BounceRates</th>\n",
       "      <th>ExitRates</th>\n",
       "      <th>PageValues</th>\n",
       "      <th>SpecialDay</th>\n",
       "      <th>Month</th>\n",
       "      <th>OperatingSystems</th>\n",
       "      <th>Browser</th>\n",
       "      <th>Region</th>\n",
       "      <th>TrafficType</th>\n",
       "      <th>VisitorType</th>\n",
       "      <th>Weekend</th>\n",
       "      <th>Revenue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Feb</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Returning_Visitor</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Feb</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Returning_Visitor</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Feb</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>Returning_Visitor</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Feb</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>Returning_Visitor</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>627.500000</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Feb</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>Returning_Visitor</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Administrative  Administrative_Duration  Informational  \\\n",
       "0               0                      0.0              0   \n",
       "1               0                      0.0              0   \n",
       "2               0                      0.0              0   \n",
       "3               0                      0.0              0   \n",
       "4               0                      0.0              0   \n",
       "\n",
       "   Informational_Duration  ProductRelated  ProductRelated_Duration  \\\n",
       "0                     0.0               1                 0.000000   \n",
       "1                     0.0               2                64.000000   \n",
       "2                     0.0               1                 0.000000   \n",
       "3                     0.0               2                 2.666667   \n",
       "4                     0.0              10               627.500000   \n",
       "\n",
       "   BounceRates  ExitRates  PageValues  SpecialDay Month  OperatingSystems  \\\n",
       "0         0.20       0.20         0.0         0.0   Feb                 1   \n",
       "1         0.00       0.10         0.0         0.0   Feb                 2   \n",
       "2         0.20       0.20         0.0         0.0   Feb                 4   \n",
       "3         0.05       0.14         0.0         0.0   Feb                 3   \n",
       "4         0.02       0.05         0.0         0.0   Feb                 3   \n",
       "\n",
       "   Browser  Region  TrafficType        VisitorType  Weekend  Revenue  \n",
       "0        1       1            1  Returning_Visitor    False    False  \n",
       "1        2       1            2  Returning_Visitor    False    False  \n",
       "2        1       9            3  Returning_Visitor    False    False  \n",
       "3        2       2            4  Returning_Visitor    False    False  \n",
       "4        3       1            4  Returning_Visitor     True    False  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INTERESTING DESCRIPTIVE STATISTICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Administrative:\n",
      "count    12330.000000\n",
      "mean         2.315166\n",
      "std          3.321784\n",
      "min          0.000000\n",
      "25%          0.000000\n",
      "50%          1.000000\n",
      "75%          4.000000\n",
      "max         27.000000\n",
      "Name: Administrative, dtype: float64\n",
      "\n",
      "Administrative_Duration:\n",
      "count    12330.000000\n",
      "mean        80.818611\n",
      "std        176.779107\n",
      "min          0.000000\n",
      "25%          0.000000\n",
      "50%          7.500000\n",
      "75%         93.256250\n",
      "max       3398.750000\n",
      "Name: Administrative_Duration, dtype: float64\n",
      "\n",
      "Informational:\n",
      "count    12330.000000\n",
      "mean         0.503569\n",
      "std          1.270156\n",
      "min          0.000000\n",
      "25%          0.000000\n",
      "50%          0.000000\n",
      "75%          0.000000\n",
      "max         24.000000\n",
      "Name: Informational, dtype: float64\n",
      "\n",
      "Informational_Duration:\n",
      "count    12330.000000\n",
      "mean        34.472398\n",
      "std        140.749294\n",
      "min          0.000000\n",
      "25%          0.000000\n",
      "50%          0.000000\n",
      "75%          0.000000\n",
      "max       2549.375000\n",
      "Name: Informational_Duration, dtype: float64\n",
      "\n",
      "ProductRelated:\n",
      "count    12330.000000\n",
      "mean        31.731468\n",
      "std         44.475503\n",
      "min          0.000000\n",
      "25%          7.000000\n",
      "50%         18.000000\n",
      "75%         38.000000\n",
      "max        705.000000\n",
      "Name: ProductRelated, dtype: float64\n",
      "\n",
      "ProductRelated_Duration:\n",
      "count    12330.000000\n",
      "mean      1194.746220\n",
      "std       1913.669288\n",
      "min          0.000000\n",
      "25%        184.137500\n",
      "50%        598.936905\n",
      "75%       1464.157213\n",
      "max      63973.522230\n",
      "Name: ProductRelated_Duration, dtype: float64\n",
      "\n",
      "BounceRates:\n",
      "count    12330.000000\n",
      "mean         0.022191\n",
      "std          0.048488\n",
      "min          0.000000\n",
      "25%          0.000000\n",
      "50%          0.003112\n",
      "75%          0.016813\n",
      "max          0.200000\n",
      "Name: BounceRates, dtype: float64\n",
      "\n",
      "ExitRates:\n",
      "count    12330.000000\n",
      "mean         0.043073\n",
      "std          0.048597\n",
      "min          0.000000\n",
      "25%          0.014286\n",
      "50%          0.025156\n",
      "75%          0.050000\n",
      "max          0.200000\n",
      "Name: ExitRates, dtype: float64\n",
      "\n",
      "PageValues:\n",
      "count    12330.000000\n",
      "mean         5.889258\n",
      "std         18.568437\n",
      "min          0.000000\n",
      "25%          0.000000\n",
      "50%          0.000000\n",
      "75%          0.000000\n",
      "max        361.763742\n",
      "Name: PageValues, dtype: float64\n",
      "\n",
      "SpecialDay:\n",
      "count    12330.000000\n",
      "mean         0.061427\n",
      "std          0.198917\n",
      "min          0.000000\n",
      "25%          0.000000\n",
      "50%          0.000000\n",
      "75%          0.000000\n",
      "max          1.000000\n",
      "Name: SpecialDay, dtype: float64\n",
      "\n",
      "Month:\n",
      "count     12330\n",
      "unique       10\n",
      "top         May\n",
      "freq       3364\n",
      "Name: Month, dtype: object\n",
      "\n",
      "OperatingSystems:\n",
      "count    12330.000000\n",
      "mean         2.124006\n",
      "std          0.911325\n",
      "min          1.000000\n",
      "25%          2.000000\n",
      "50%          2.000000\n",
      "75%          3.000000\n",
      "max          8.000000\n",
      "Name: OperatingSystems, dtype: float64\n",
      "\n",
      "Browser:\n",
      "count    12330.000000\n",
      "mean         2.357097\n",
      "std          1.717277\n",
      "min          1.000000\n",
      "25%          2.000000\n",
      "50%          2.000000\n",
      "75%          2.000000\n",
      "max         13.000000\n",
      "Name: Browser, dtype: float64\n",
      "\n",
      "Region:\n",
      "count    12330.000000\n",
      "mean         3.147364\n",
      "std          2.401591\n",
      "min          1.000000\n",
      "25%          1.000000\n",
      "50%          3.000000\n",
      "75%          4.000000\n",
      "max          9.000000\n",
      "Name: Region, dtype: float64\n",
      "\n",
      "TrafficType:\n",
      "count    12330.000000\n",
      "mean         4.069586\n",
      "std          4.025169\n",
      "min          1.000000\n",
      "25%          2.000000\n",
      "50%          2.000000\n",
      "75%          4.000000\n",
      "max         20.000000\n",
      "Name: TrafficType, dtype: float64\n",
      "\n",
      "VisitorType:\n",
      "count                 12330\n",
      "unique                    3\n",
      "top       Returning_Visitor\n",
      "freq                  10551\n",
      "Name: VisitorType, dtype: object\n",
      "\n",
      "Weekend:\n",
      "count     12330\n",
      "unique        2\n",
      "top       False\n",
      "freq       9462\n",
      "Name: Weekend, dtype: object\n",
      "\n",
      "Revenue:\n",
      "count     12330\n",
      "unique        2\n",
      "top       False\n",
      "freq      10422\n",
      "Name: Revenue, dtype: object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for each in df.columns:\n",
    "    print(each + \":\")\n",
    "    print(df[each].describe(), end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create lists of categorical/continuous features for visualizations and piplining\n",
    "categorical_feats = df.columns.tolist()[12:-1]\n",
    "numerical_feats = df.columns.tolist()[:12]\n",
    "#some housekeeping to keep the months in order in visualizations\n",
    "\n",
    "months = [\"Feb\", \"Mar\", \"May\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
    "df['Month'] = pd.Categorical(df['Month'], categories=months, ordered=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for each in categorical_feats:\n",
    "#     fig = px.histogram(df, x=each, color='Revenue', )\n",
    "#     fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#to simplify visualizations\n",
    "df[\"Invert_Revenue\"] = (df['Revenue']==0).astype(int)\n",
    "\n",
    "# for each in categorical_feats:\n",
    "#     categories = df[each].unique().tolist()\n",
    "#     categories.sort    \n",
    "#     sales = df.groupby(each)['Revenue'].sum().tolist()\n",
    "#     passes = df.groupby(each)['Invert_Revenue'].sum().tolist()\n",
    "\n",
    "#     fig = go.Figure()\n",
    "#     fig.add_bar(x=categories, y=passes, name='No Sale').add_bar(x=categories, y=sales, name='Sale').update_layout(title_text = each, barmode='stack')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for each in categorical_feats:\n",
    "#     categories = df[each].unique().tolist()\n",
    "#     categories.sort()\n",
    "\n",
    "#     total = df.groupby(each)['Revenue'].count().tolist()\n",
    "#     sales = df.groupby(each)['Revenue'].sum().tolist()\n",
    "#     passes = df.groupby(each)['Invert_Revenue'].sum().tolist()\n",
    "#     sale_percent = []\n",
    "#     pass_percent = []\n",
    "#     for idx, category in enumerate(categories):\n",
    "#         sale_percent.append(sales[idx] / total[idx] *100)\n",
    "#         pass_percent.append(passes[idx]/total[idx] * 100)\n",
    "\n",
    "#     fig = go.Figure()\n",
    "#     fig.add_bar(x=categories, y=pass_percent, name='No Sale').add_bar(x=categories, y=sale_percent, name='Sale').update_layout(title_text = each, barmode='stack')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for each in numerical_feats:\n",
    "#     fig = px.scatter(df, x=each, y='Revenue')\n",
    "#     fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#  for each in numerical_feats:\n",
    "#      fig = px.scatter(df, x=each, y='Total_Duration', color='Revenue')\n",
    "#      fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I added the total page views and the total time spent on the site as additional features. I expected that these features would correlate strongly with Producted Related counts and duration, but thought that it was useful to capture how the user interacted with the overall site during their session. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#create new features by summing existing features\n",
    "df['Total_Pages'] = df.apply(lambda row: row.Administrative + row.Informational + row.ProductRelated, axis=1)\n",
    "df['Total_Duration'] = df.apply(lambda row: row.Administrative_Duration + row.Informational_Duration + row.ProductRelated_Duration, axis=1)\n",
    "\n",
    "#move the new columns to a place in the dataset where I prefer them\n",
    "cols = df.columns.tolist()\n",
    "new_cols = cols[:6] + cols[-2:] + cols[6:-2]\n",
    "\n",
    "df = df[new_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I considered the possibility that the site the data represented was new, or had been newly linked to Google Analytics for the purposes of the project. I wanted to see if the Google Analytics features would change over time as more users interacted with the site. After printing out the following visualizations, it seems that the distribution of data over time parallels that of other features plotted against Month. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if visualizations:    \n",
    "    fig = make_subplots(rows=3, cols=1)\n",
    "\n",
    "    fig.append_trace(go.Scatter(\n",
    "        x = df[\"Month\"], y=df[\"BounceRates\"],\n",
    "        name='Bounce Rates',\n",
    "        mode='markers',\n",
    "        marker_color = '#22577a'\n",
    "    ), row=1, col=1)\n",
    "\n",
    "    fig.append_trace(go.Scatter(\n",
    "        x = df[\"Month\"], y=df[\"ExitRates\"],\n",
    "        name='Exit Rates',\n",
    "        mode='markers',\n",
    "        marker_color='#38a3a5'\n",
    "    ), row=2, col=1)\n",
    "\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x = df[\"Month\"], y=df[\"PageValues\"],\n",
    "        name='Page Values',\n",
    "        mode='markers',\n",
    "        marker_color='#57cc99'\n",
    "    ), row=3, col=1)\n",
    "\n",
    "    fig.update_layout(hovermode=False)\n",
    "    #this line of code took me 45 minutes rip\n",
    "    fig.update_xaxes(categoryorder = 'array', categoryarray=np.array(months))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reading the scant literature on the conceptualization of the 'Special Day' feature, I wanted to explore it further. Is there a 'Special Day' for each month? Which days are 'Special'? Do they correlate with sales peaks?\n",
    "\n",
    "My investigation was short, as it turns out there were only two 'Special Days', and they were those named in the study: Valentine's Day and Mother's Day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Month\n",
       "Feb    0.233696\n",
       "Mar    0.000000\n",
       "May    0.212366\n",
       "Jul    0.000000\n",
       "Aug    0.000000\n",
       "Sep    0.000000\n",
       "Oct    0.000000\n",
       "Nov    0.000000\n",
       "Dec    0.000000\n",
       "Name: SpecialDay, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('Month')['SpecialDay'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As attentive readers may have noticed, this dataset is missing several months' worth of data! We have no data for January, April, or June. In addition to having incomplete monthly data, this also renders the overall counts from month-to-month suspect, ad we're not sure if certain months are lower count due to lower traffic, or due to errors in data collection. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several of my categorical variables has categories with counts so low that when the data was split into training and test data, some categories wouldn't appear in the test data. For consistency across the training and test data, I grouped low-count categories into a single 'other' (coded as '99', since the categories were all integers). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#columns with low-count categories\n",
    "sm_outliers = ['OperatingSystems', 'Browser', 'TrafficType']\n",
    "\n",
    "#this loop codes any low-count category as a single replacement value. I chose 99\n",
    "for each in sm_outliers:\n",
    "    #code snippet below from stackoverflow\n",
    "    series = pd.value_counts(df[each])\n",
    "    mask = (series/series.sum() * 100).lt(1)\n",
    "    # To replace df['column'] use np.where I.e \n",
    "    df[each] = np.where(df[each].isin(series[mask].index),99,df[each])\n",
    "    #print(df[each].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ensure that target variable is typed correctly\n",
    "df['Revenue'] = df['Revenue'].astype('int32')\n",
    "\n",
    "#cast other binary categorical variable as the correct type\n",
    "df['Weekend'] = df['Weekend'].astype(int)\n",
    "\n",
    "#remove Invert Revenue column, otherwise it helps the models\n",
    "if visualizations:\n",
    "    df.drop('Invert_Revenue', axis=1, inplace=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I did the test-train split, separating out 20% of my data to test my models for overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('Revenue', axis=1), df['Revenue'], test_size=0.2, random_state=270)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although I initially wanted to use a Pipeline for my modeling, I found that I needed to use the SMOTE sampling strategy to balance my data. The SMOTE tool doesn't work into a sklearn Pipeline, and the imblearn Pipeline doesn't work with sklearn models. For that reason, I created dummy variables and scaled the data by hand, and saved those values in separate variables from those I entered into the Pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Nov'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-1e563f569dbb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m#fit the scaler on the training data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mx_scale\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessed_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m#create scaled X-values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    665\u001b[0m         \u001b[1;31m# Reset internal state before fitting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    666\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 667\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    668\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    669\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    694\u001b[0m             \u001b[0mTransformer\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    695\u001b[0m         \"\"\"\n\u001b[1;32m--> 696\u001b[1;33m         X = self._validate_data(X, accept_sparse=('csr', 'csc'),\n\u001b[0m\u001b[0;32m    697\u001b[0m                                 \u001b[0mestimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m                                 force_all_finite='allow-nan')\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    418\u001b[0m                     \u001b[1;34mf\"requires y to be passed, but the target y is None.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    419\u001b[0m                 )\n\u001b[1;32m--> 420\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    421\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    422\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    596\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"unsafe\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 598\u001b[1;33m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    599\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m     \"\"\"\n\u001b[1;32m---> 85\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1780\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1781\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1782\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1783\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__array_wrap__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m     \"\"\"\n\u001b[1;32m---> 85\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'Nov'"
     ]
    }
   ],
   "source": [
    "#create dummies for categorical features\n",
    "processed_train = pd.get_dummies(X_train, columns=categorical_feats, drop_first = True)\n",
    "processed_test = pd.get_dummies(X_test, columns=categorical_feats, drop_first = True)\n",
    "\n",
    "#save the columns for later\n",
    "processed_columns = processed_train.columns.to_list()\n",
    "\n",
    "#fit the scaler on the training data\n",
    "x_scale = StandardScaler().fit(processed_train)\n",
    "\n",
    "#create scaled X-values\n",
    "processed_train = x_scale.transform(processed_train)\n",
    "processed_test = x_scale.fit_transform(processed_test)\n",
    "\n",
    "\n",
    "#I'm still not sure if I should be scaling my targets or not?\n",
    "#Most of the models didn't like having scaled models\n",
    "#if I need it in a pinch, it's here.\n",
    "\n",
    "#processed_y_train = y_train.to_numpy().reshape(-1,1)\n",
    "#y_scale = StandardScaler().fit(processed_y_train)\n",
    "#processed_y_train = y_scale.transform(processed_y_train)\n",
    "#processed_y_test = y_test.to_numpy().reshape(-1,1)\n",
    "#processed_y_test = y_scale.transform(processed_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I did have some occasion to use the sklearn Pipeline, so I constructed one below. The Pipeline accepts separate categorical and numerical data for preprocessing, then joins the two in the ColumnTransformer section of the pipe, which will be invoked alongside the given model. \n",
    "\n",
    "<i>I didn't end up using this Pipeline as much as I anticipated given the above complication with SMOTE. </i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numerical pipe, scales data\n",
    "num_pipe = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "#categorical pipe, one-hot-encodes and scales data\n",
    "cat_pipe = Pipeline(steps=[\n",
    "    ('ohe', OneHotEncoder(drop='first')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "#combines numerical and categorical data\n",
    "transformer = ColumnTransformer(transformers=[\n",
    "    ('num', num_pipe, numerical_feats),\n",
    "    ('cat', cat_pipe, categorical_feats)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I used SMOTE to create synthetic data points with a positive target (sales) to balance out the negative values (sessions without a sale). Because the class imbalance in my model was pretty dramatic, SMOTE proved to be a really crucial tool. The difference between balanced and unbalanced data will be apparent in the modeling section.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use SMOTE to create synthetic columns so that there are an equal number of positive/negative values\n",
    "smote = SMOTE()\n",
    "X_train_resampled, y_train_resampled = smote.fit_sample(processed_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_pipe = Pipeline(steps=[\n",
    "    ('ColumnTransformer', transformer),\n",
    "    ('Classifier', LogisticRegression(random_state=270, solver='lbfgs', max_iter=250))\n",
    "])\n",
    "\n",
    "lr_pipe.fit(X_train, y_train)\n",
    "\n",
    "print(lr_pipe.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_test = lr_pipe.predict(X_test)\n",
    "y_hat_train = lr_pipe.predict(X_train)\n",
    "residuals = np.abs(y_train - y_hat_train)\n",
    "print('Train residuals:')\n",
    "print(pd.Series(residuals).value_counts())\n",
    "print(pd.Series(residuals).value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = np.abs(y_test - y_hat_test)\n",
    "print('Test residuals:')\n",
    "print(pd.Series(residuals).value_counts())\n",
    "print(pd.Series(residuals).value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_train = confusion_matrix(y_train, y_hat_train)\n",
    "ax = sns.heatmap(cf_train/np.sum(cf_train), annot=True, fmt='.2%', cmap = 'Blues')\n",
    "ax.set(xlabel='True', ylabel='Predicted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_test = confusion_matrix(y_test, y_hat_test)\n",
    "ax = sns.heatmap(cf_test/np.sum(cf_test), annot=True, fmt='.2%', cmap = 'Blues')\n",
    "ax.set(xlabel='True', ylabel='Predicted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = ['No Sale', 'Sale']\n",
    "print(classification_report(y_train, y_hat_train, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_pipe = Pipeline(steps=[\n",
    "    ('ColumnTransformer', transformer),\n",
    "    ('Classifier', LogisticRegression(class_weight='balanced', random_state=270, solver='lbfgs', max_iter=250))\n",
    "])\n",
    "\n",
    "lr_pipe.fit(X_train, y_train)\n",
    "\n",
    "print(lr_pipe.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_train = lr_pipe.predict(X_train)\n",
    "cf_train = confusion_matrix(y_train, y_hat_train)\n",
    "ax = sns.heatmap(cf_train/np.sum(cf_train), annot=True, fmt='.2%', cmap = 'Blues')\n",
    "ax.set(xlabel='True', ylabel='Predicted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_lr = LogisticRegression(random_state=270, solver='lbfgs', max_iter=250)\n",
    "manual_lr.fit(X_train_resampled, y_train_resampled)\n",
    "y_hat_train = manual_lr.predict(processed_train)\n",
    "y_hat_test = manual_lr.predict(processed_test)\n",
    "y_score = manual_lr.decision_function(processed_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score = lr_pipe.fit(X_train, y_train).decision_function(X_test)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_score)\n",
    "\n",
    "print('AUC: {}'.format(auc(fpr, tpr)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAKE THIS PRETTIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ax = sns.lineplot(x=fpr, y=tpr, color = 'red')\n",
    "ax = sns.lineplot(x=[0,1], y=[0,1], color='navy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Would prefer a model that predicted sales better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_pipe = Pipeline(steps=[\n",
    "    ('ColumnTransformer', transformer),\n",
    "    ('Classifier', tree.DecisionTreeClassifier(criterion='entropy'))\n",
    "])\n",
    "\n",
    "dt_pipe.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc = tree.DecisionTreeClassifier(criterion='entropy')\n",
    "\n",
    "dtc.fit(processed_train, y_train.to_numpy().reshape(-1,1))\n",
    "\n",
    "tree.plot_tree(dtc,\n",
    "             feature_names = processed_columns,\n",
    "             class_names = np.unique(y_train).astype('str'),\n",
    "             filled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_train = dtc.predict(processed_train)\n",
    "y_hat_test = dtc.predict(processed_test)\n",
    "\n",
    "print(classification_report(y_train, y_hat_train, target_names=target_names))\n",
    "print(classification_report(y_test, y_hat_test, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dtc = tree.DecisionTreeClassifier(criterion='entropy', max_depth = 5)\n",
    "\n",
    "dtc.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "tree.plot_tree(dtc,\n",
    "             feature_names = processed_columns,\n",
    "             class_names = np.unique(y_train).astype('str'),\n",
    "             filled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_train = dtc.predict(X_train_resampled)\n",
    "y_hat_test = dtc.predict(processed_test)\n",
    "\n",
    "print(classification_report(y_train_resampled, y_hat_train, target_names=target_names))\n",
    "print(classification_report(y_test, y_hat_test, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc_search = tree.DecisionTreeClassifier()\n",
    "\n",
    "param_grid = {\n",
    "    'criterion' : ['gini', 'entropy'],\n",
    "    'max_depth' : [2, 5, 7, 10],\n",
    "    'min_samples_split' : [2, 5, 10, 20]\n",
    "}\n",
    "\n",
    "gs_tree = GridSearchCV(dtc_search, param_grid, cv=3)\n",
    "gs_tree.fit(X_train_resampled, y_train_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gs_tree.best_params_)\n",
    "\n",
    "y_hat_train = gs_tree.predict(processed_train)\n",
    "y_hat_test = gs_tree.predict(processed_test)\n",
    "\n",
    "print(classification_report(y_train, y_hat_train, target_names=target_names))\n",
    "print(classification_report(y_test, y_hat_test, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
